{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "Random Forest - Best Hyperparameters: {'n_estimators': 110, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_depth': 14, 'criterion': 'gini', 'bootstrap': False}\n",
      "Random Forest - Best Accuracy: 0.9733333333333334\n",
      "SVM - Best Hyperparameters: {'kernel': 'rbf', 'gamma': 0.04563716281924759, 'degree': 1, 'C': 5.72236765935022}\n",
      "SVM - Best Accuracy: 0.9800000000000001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Standardize features for SVM\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Define the hyperparameter grid for RandomizedSearchCV for Random Forest\n",
    "param_dist_rf = {\n",
    "    'n_estimators': np.arange(10, 200, 10),\n",
    "    'max_depth': [None] + list(np.arange(1, 20)),\n",
    "    'min_samples_split': np.arange(2, 11),\n",
    "    'min_samples_leaf': np.arange(1, 11),\n",
    "    'bootstrap': [True, False],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Define the hyperparameter grid for RandomizedSearchCV for SVM\n",
    "param_dist_svm = {\n",
    "    'C': np.logspace(-3, 3, 100),  # Regularization parameter\n",
    "    'kernel': ['linear', 'rbf', 'poly'],  # Kernel types\n",
    "    'degree': np.arange(1, 6),  # Degree of the polynomial kernel (for poly kernel)\n",
    "    'gamma': ['scale', 'auto'] + list(np.logspace(-3, 3, 500)),  # Kernel coefficient (for rbf and poly kernels)\n",
    "}\n",
    "\n",
    "# Create a K-Fold cross-validation object\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Create a RandomizedSearchCV object for Random Forest\n",
    "random_search_rf = RandomizedSearchCV(\n",
    "    estimator=RandomForestClassifier(),\n",
    "    param_distributions=param_dist_rf,\n",
    "    n_iter=500,  # Increase the number of attempts to 500\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,  # Use all available CPU cores for parallelism\n",
    "    cv=kf,  # Use K-Fold cross-validation\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create a RandomizedSearchCV object for SVM\n",
    "random_search_svm = RandomizedSearchCV(\n",
    "    estimator=SVC(),\n",
    "    param_distributions=param_dist_svm,\n",
    "    n_iter=500,  # Increase the number of attempts to 500\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,  # Use all available CPU cores for parallelism\n",
    "    cv=kf,  # Use K-Fold cross-validation\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the RandomizedSearchCV for Random Forest to the data\n",
    "random_search_rf.fit(X, y)\n",
    "\n",
    "# Fit the RandomizedSearchCV for SVM to the data\n",
    "random_search_svm.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters and corresponding accuracy for Random Forest\n",
    "print(\"Random Forest - Best Hyperparameters:\", random_search_rf.best_params_)\n",
    "print(\"Random Forest - Best Accuracy:\", random_search_rf.best_score_)\n",
    "\n",
    "# Print the best hyperparameters and corresponding accuracy for SVM\n",
    "print(\"SVM - Best Hyperparameters:\", random_search_svm.best_params_)\n",
    "print(\"SVM - Best Accuracy:\", random_search_svm.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "Random Forest - Best Hyperparameters: {'n_estimators': 100, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_depth': 17, 'criterion': 'entropy', 'bootstrap': False}\n",
      "Random Forest - Best Accuracy: 0.9626373626373628\n",
      "Random Forest - Test Accuracy: 0.9649122807017544\n",
      "\n",
      "Gradient Boosting - Best Hyperparameters: {'n_estimators': 60, 'min_samples_split': 6, 'min_samples_leaf': 6, 'max_depth': 3, 'learning_rate': 0.5}\n",
      "Gradient Boosting - Best Accuracy: 0.964835164835165\n",
      "Gradient Boosting - Test Accuracy: 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid for RandomizedSearchCV for Random Forest\n",
    "param_dist_rf = {\n",
    "    'n_estimators': np.arange(10, 200, 10),\n",
    "    'max_depth': [None] + list(np.arange(1, 20)),\n",
    "    'min_samples_split': np.arange(2, 11),\n",
    "    'min_samples_leaf': np.arange(1, 11),\n",
    "    'bootstrap': [True, False],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Define the hyperparameter grid for RandomizedSearchCV for Gradient Boosting\n",
    "param_dist_gb = {\n",
    "    'n_estimators': np.arange(10, 200, 10),\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.3, 0.5],\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8],\n",
    "    'min_samples_split': np.arange(2, 11),\n",
    "    'min_samples_leaf': np.arange(1, 11)\n",
    "}\n",
    "\n",
    "# Create a K-Fold cross-validation object\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Create a RandomizedSearchCV object for Random Forest\n",
    "rf_classifier = RandomForestClassifier()\n",
    "random_search_rf = RandomizedSearchCV(\n",
    "    estimator=rf_classifier,\n",
    "    param_distributions=param_dist_rf,\n",
    "    n_iter=500,  # Increase the number of attempts to 500\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,  # Use all available CPU cores for parallelism\n",
    "    cv=kf,  # Use K-Fold cross-validation\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create a RandomizedSearchCV object for Gradient Boosting\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "random_search_gb = RandomizedSearchCV(\n",
    "    estimator=gb_classifier,\n",
    "    param_distributions=param_dist_gb,\n",
    "    n_iter=500,  # Increase the number of attempts to 500\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,  # Use all available CPU cores for parallelism\n",
    "    cv=kf,  # Use K-Fold cross-validation\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the RandomizedSearchCV for Random Forest to the training data\n",
    "random_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Fit the RandomizedSearchCV for Gradient Boosting to the training data\n",
    "random_search_gb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the models on the test set and print accuracy\n",
    "rf_pred = random_search_rf.predict(X_test)\n",
    "gb_pred = random_search_gb.predict(X_test)\n",
    "\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "\n",
    "print(\"Random Forest - Best Hyperparameters:\", random_search_rf.best_params_)\n",
    "print(\"Random Forest - Best Accuracy:\", ((random_search_rf.best_score_)*100).round(2))\n",
    "print(\"Random Forest - Test Accuracy:\", ((rf_accuracy)*100).round(2))\n",
    "\n",
    "print(\"\\nGradient Boosting - Best Hyperparameters:\", random_search_gb.best_params_)\n",
    "print(\"Gradient Boosting - Best Accuracy:\", ((random_search_gb.best_score_)*100).round(2))\n",
    "print(\"Gradient Boosting - Test Accuracy:\", ((gb_accuracy)*100).round(2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
